<!DOCTYPE html>
<html>

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XB3PR2Y1TQ"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-XB3PR2Y1TQ');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition</title>
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="assets/css/Highlight-Clean.css">
    <link rel="stylesheet" href="assets/css/styles.css">
    <link rel="icon" href="./sources/radarv1.3.png">


    <meta property="og:site_name" content="WeakSAM" />
    <meta property="og:type" content="video.other" />
    <meta property="og:title" content="WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition" />
    <meta property="og:description" content="WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition, 2024." />
    <meta property="og:url" content="" />

    <script src="assets/js/video_comparison.js"></script>
    <script type="module" src="https://unpkg.com/@google/model-viewer@2.0.1/dist/model-viewer.min.js"></script>

    <style>
        .image-container {
          display: flex;
          justify-content: space-between;
        }
        .image-container img {
          width: 50%; /* 每张图片宽度设置为50% */
        }
      </style>

</head>


<body>

    
    <!-- <div class="container" style="max-width: 500px;">
        <div class="row captioned_videos">
            <div class="col-md-12">
                <img src="sources/vis_det_v1.6.png" alt="architecture" style="width: 100%">
            </div>
        </div>
    </div> -->

    <div class="highlight-clean" style="padding-bottom: 10px;">
        <div class="container" style="max-width: 1024px; margin-bottom: 20px;">
            <h1 class="text-center"><b>WeakSAM</b>:  Segment Anything Meets Weakly-supervised Instance-level Recognition</h1>
        </div>
        <div class="container" style="max-width: 1024px; margin-bottom: 20px;">
            <h2 class="text-center"><b>ACM Multimedia 2024
        </div>

        <div class="container" style="max-width: 980px; margin-bottom: 20px;">
            <div class="row authors">
                <div class="col">
                    <!-- <h5 class="text-center" style="text-align: center">Authors anonymized</h5> -->
                    <div class="text-center">
                            <span class="author-block" style="font-size: 20px;">
                            <a href="https://github.com/Unrealluver">Lianghui Zhu</a><sup>1*</sup>,</span>
                            <span class="author-block" style="font-size: 20px;">
                            <a href="https://colezwhy.github.io/">Junwei Zhou</a><sup>1*</sup>,</span>
                            <span class="author-block" style="font-size: 20px;">
                            <a> Yan Liu </a><sup>2</sup>,</span>
                          <span class="author-block" style="font-size: 20px;">
                          <a> Xin Hao </a><sup>2</sup>,</span>
                    </div>
                    <div class="text-center">
                        <!-- <p></p> -->
                        <span class="author-block" style="font-size: 20px;">

                          <a href="https://scholar.google.com/citations?user=D7jDk7gAAAAJ&hl=zh-CN">Wenyu Liu</a><sup>1</sup>,
                        </span>
                        <span class="author-block" style="font-size: 20px;">

                          <a href="https://xwcv.github.io/">Xinggang Wang</a><sup>1✉</sup>
                        </span>
                      </div>
            
                      <div class="text-center">
                        <span class="author-block"><sup>1</sup>School of EIC, Huazhong University of Science and Technology</span>
                        <span class="author-block"><sup>2</sup>Alipay Tian Qian Security Lab</span>
                        <!-- <p></p> -->
                      </div>
                        <div class="text-center">
                        <span class="author-block"><sup>*</sup>Equal contribution.</span>
                        <span class="author-block"><sup>✉</sup>Corresponding author.</span>
                    </div>
                </div>
            </div>
            <div class="buttons" style="margin-top: 8px; margin-bottom: 8px;">
                <a class="btn btn-light" role="button" href="https://arxiv.org/abs/2402.14812">
                    <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                        <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"></path>
                    </svg>Paper
                </a>
                <a class="btn btn-light" role="button" href="https://github.com/hustvl/WeakSAM">
                    <svg xmlns="http://www.w3.org/2000/svg" style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                        <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 
                                1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 
                                3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 
                                2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path>
                    </svg>
                    Code
                </a>
                <!-- <a class="btn btn-light" role="button" href="https://huggingface.co/spaces/thewhole/GaussianDreamer_Demo">
                <img src="./static/hf-logo.png" alt="" style="width:24px;height:24px;margin-left:-12px;margin-right:12px">
                    Huggingface demo
                </a> -->
            </div>
    </div>

    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Abstract</h2>
                <p>
                    Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multiinstance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weaklysupervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM’s problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with large margins, i.e. average improvements of 7.4% and 8.5%, respectively.
                </p>
            </div>
        </div>
    </div>
    <div class="container" style="max-width: 384px;">
        <div class="row captioned_videos">
            <div class="col-md-12">
                <img src="sources/radarv1.3.png" alt="architecture" style="width: 100%">
                
            </div>
        </div>
    </div>

    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Weaknesses in weakly-supervised instance-level recognition </h2>

                <h3>Low-quality proposals in WSOD</h3>

                <p> This figure shows the cosine similarity among the features of proposals, i.e., Left for Selective Search proposals and Right for WeakSAM-proposals. For a single image from PASCAL VOC 2007, we randomly sampled 200 proposal features to calculate their similarity. This suggests that the usually used proposal set in WSOD show great redundancy, which hinders the optimization process of WSOD models.</p>
                <div class="container" style="max-width: 550px;">
                    <div class="row">
                        <div class="col-md-12">
                        <img src="sources/proposal_comp_v1.1.png" alt="architecture" style="width: 100%">
                        </div>
                    </div>
                </div>
                
                <br><br>

                <h3>Noisy pseudo ground truth(PGT) in retraining</h3>
                <p> The figures below show the relationship between the normalized classification and regression loss, the corresponding number of RoIs, and the corresponding error rate. The results are obtained from training the Faster-RCNN using PGT in the preliminary training stage.</p>
                
                <div class="image-container">
                <img src="sources/instance_grad_cls_v1.5.png" alt="architecture">
                <img src="sources/loss_drop_reg_v1.1.png" alt="architecture">
                </div>
            </div>
        </div>
    </div>

    <br><br>
    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Proposed Framework & Method</h2>
                <p>
                    An overview of the proposed WeakSAM framework. We first generate activation maps from a classification ViT. Subsequently, we introduce classification clues and spatial points as automatic WeakSAM prompts, which address the problem of SAM requiring interactive prompts. Next, we use the WeakSAM proposals in the WSOD pipeline, in which the weakly-supervised detector performs class-aware perception to annotate pseudo ground truth (PGT). Then, we analyze the incompleteness and the noise problem existing in PGT and propose adaptive PGT generation, RoI drop regularization to address them, respectively. Finally, we launch WSIS training supervised by pseudo instance labels, which requires adaptive PGT as SAM prompts. The snowflake mark means the model is frozen.
                </p>
            </div>
        </div>
    </div>
    <div class="container" style="max-width: 768px;">
        <div class="row captioned_videos">
            <div class="col-md-12">
                <img src="sources/weaksam_pipeline_v4.0_1.png" alt="architecture" style="width: 100%">
            </div>
        </div>
    </div>


    <!-- <div class="container" style="max-width: 500px;">
        <div class="row captioned_videos">
            <div class="col-md-12">
                <img src="sources/algo1.png" alt="architecture" style="width: 100%">

            </div>
        </div>
    </div> -->
    

    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-sm-12">
                <h2>Experiments & Main Results</h2>
                <p>We evaluate the proposed WeakSAM on both weakly-supervised object detection (WSOD) and weakly-supervised instance segmentation (WSIS) benchmarks. Notably, the same datasets for different tasks may have different settings. For WSOD, we use three datasets, i.e., PASCAL VOC 2007, PASCAL VOC 2012, and COCO 2014.</a>.</a></p>
            </div>
        </div>


        <div class="container" style="max-width: 768px;">
            <div class="row captioned_videos">
                <div class="col-md-12">
                    <p> Comparisons of the WSOD performance in terms of AP metrics on three benchmarks: PASCAL VOC 2007, PASCAL VOC 2012, and COCO 2014. The Sup. column denotes the type of supervision used for training including full supervision (F), point-level labels (P), image-level labels (I). “*” means the results rely on MCG proposals. “‡” means this method use the a heavy RN50-WS-MRRP backbone (1.76 × parameters than VGG16 and 10.10 × parameters than RN50). We mark the best WSOD results in bold.</p>
                    <img src="sources/wsod.png" alt="architecture" style="width: 100%">
                </div>
            </div>
        </div>

        <br><br>

        <div class="container" style="max-width: 768px;">
            <div class="row captioned_videos">
                <div class="col-md-12">
                    <p> Comparisons of the WSIS performance in terms of AP metrics on PASCAL VOC 2012. The Sup. column denotes the type of supervision used for training including mask supervision (M), saliency maps (S), image-level labels (I), and SAM models (A). We mark the best WSIS results in bold. </p>
                    <img src="sources/wsis_voc.png" alt="architecture" style="width: 100%">
                </div>
            </div>
        </div>

        <br><br>

        <div class="container" style="max-width: 768px;">
            <div class="row captioned_videos">
                <div class="col-md-12">
                    <p> Comparisons of the WSOD performance in terms of AP metrics on three benchmarks: PASCAL VOC 2007, PASCAL VOC 2012, and COCO 2014. The Sup. column denotes the type of supervision used for training including full supervision (F), point-level labels (P), image-level labels (I). “*” means the results rely on MCG proposals. “‡” means this method use the a heavy RN50-WS-MRRP backbone (1.76 × parameters than VGG16 and 10.10 × parameters than RN50). We mark the best WSOD results in bold.</p>
                    <img src="sources/wsis_coco.png" alt="architecture" style="width: 100%">
                </div>
            </div>
        </div>
    </div>

    
    <!-- <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-sm-12">
                <h2>Ablations</h2>
                <p>We use the point clouds with the added ground to initialize the 3D Gaussians.</a>.</p>
            </div>
        </div>

    </div> -->

    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-sm-12">
                <h2>Visualization Results</h2>
                <p>Some visualization results of our WeakSAM.</p>
            </div>
        </div>

        <div class="container" style="max-width: 768px;">
            <div class="row captioned_videos">
                <div class="col-md-12">
                    <p> Visualization of the weakly-supervised object detection on the PASCAL VOC 2007 test set.</p>
                    <img src="sources/vis_det_v1.6.png" alt="architecture" style="width: 100%">
                </div>
            </div>
        </div>
        
        <br><br>

        <div class="container" style="max-width: 768px;">
            <div class="row captioned_videos">
                <div class="col-md-12">
                    <p> Visualization of the weakly-supervised instance segmentation on the PASCAL VOC 2012 val set.</p>
                    <img src="sources/vis_ins_seg_v1.3.png" alt="architecture" style="width: 100%">
                </div>
            </div>
        </div>

        <br><br>

        <div class="container" style="max-width: 768px;">
            <div class="row captioned_videos">
                <div class="col-md-12">
                    <p> Visualization of the pseudo ground truth boxes and pseudo instance labels on the PASCAL VOC 2012 trainaug set.</p>
                    <img src="sources/vis_pgt_v1.1.png" alt="architecture" style="width: 100%">
                </div>
            </div>
        </div>
        
        <br><br>

        <div class="container" style="max-width: 768px;">
            <div class="row captioned_videos">
                <div class="col-md-12">
                    <p> Visualization of the Selective Search and WeakSAM proposals boxes on the PASCAL VOC 2007 trainval set.</p>
                    <img src="sources/vis_proposals_v1.0.png" alt="architecture" style="width: 100%">
                </div>
            </div>
        </div>
        
    </div>

    

    <!-- <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-sm-12">
                <h2>Paint the SMPL</h2>
                <p>Generate examples using the SMPL initialization. The SMPL is generated using text prompt through <a href="https://guytevet.github.io/mdm-page/">MDM</a>.</p>
            </div>
        </div>

        <div class="row captioned_videos">
            <div class="col-4">
                <div class="video-compare-container" style="width: 100% ">
                    <div style="display: flex; justify-content: center;">
                        <img src="static/kick.png" alt="architecture" style="width: 45%">
                    </div>
                    <h6 class="caption">Someone kicks with his left leg</h6>
                </div>
            </div>

            <div class="col-4">
                <div class="video-compare-container" style="width: 100%">
                    <video class="video lazy" loop playsinline autoplay muted style="width: 100%">
                        <source data-src="./human/iron.mp4" type="video/mp4"></source>
                    </video>
                    <h6 class="caption">Iron man kicks with his left leg</h6>
                </div>
            </div>
            <div class="col-4">
                <div class="video-compare-container" style="width: 100%">
                    <video class="video lazy" loop playsinline autoplay muted style="width: 100%">
                        <source data-src="./human/hulk.mp4" type="video/mp4"></source>
                    </video>
                    <h6 class="caption">Hulk kicks with his left leg</h6>
                </div>
                
            </div>
            
            <div class="row captioned_videos">
                <div class="col-4">
                    <div class="video-compare-container" style="width: 100% ">
                        <div style="display: flex; justify-content: center;">
                            <img src="static/jump.png" alt="architecture" style="width: 44%">
                        </div>
                        <h6 class="caption">The man jumped down from the sky</h6>
                    </div>
                </div>
    
                <div class="col-4">
                    <div class="video-compare-container" style="width: 100%">
                        <video class="video lazy" loop playsinline autoplay muted style="width: 100%">
                            <source data-src="./human/link.mp4" type="video/mp4"></source>
                        </video>
                        <h6 class="caption">Link in Zelda jumped down from the sky</h6>
                    </div>
                    
                </div>
                <div class="col-4">
                    <div class="video-compare-container" style="width: 100%">
                        <video class="video lazy" loop playsinline autoplay muted style="width: 100%">
                            <source data-src="./human/batman.mp4" type="video/mp4"></source>
                        </video>
                        <h6 class="caption">Batman jumped down from the sky</h6>
                    </div>
                    
                </div>

        </div>
    </div> -->
    

    <!-- <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-sm-12">
                <h2>Application</h2>
                <p>Import the generated 3D assets into the Unity game engine to become materials for games and designs with the help of <a href="https://github.com/aras-p/UnityGaussianSplatting">UnityGaussianSplatting
                </a>.</p>
            </div>
        </div>

        <div class="row captioned_videos">


            <div class="col-6">
                <div class="video-compare-container" style="width: 100%">
                    <video class="video lazy" loop playsinline autoplay muted style="width: 100%">
                        <source data-src="./unity/3d.mp4" type="video/mp4"></source>
                    </video>
                    <h6 class="caption">Generated by GaussianDreamer.</h6>
                </div>
            </div>

            
            <div class="col-6">
                <div class="video-compare-container" style="width: 130%">
                    <video class="video lazy" loop playsinline autoplay muted style="width: 100%">
                        <source data-src="./unity/unity.mp4"  type="video/mp4"></source>
                    </video>
                    <h6 class="caption">Import the generated 3D assets into the Unity game engine.</h6>
                </div>
            </div>
    


        </div>
    </div> -->
    
    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-sm-12">
                <h2>More Research and Related Works</h2>
                <span class="author-block" style="font-size: 20px;">
                    <!-- <h1 class="text-center">sdsad</h1> -->
                    <a class="navbar-item" href="https://github.com/hustvl/WeakTr">
                        WeakTr,
                      </a>
                      <a class="navbar-item" href="https://github.com/NVlabs/wetectron">
                        MIST,
                      </a>
                      <a class="navbar-item" href="https://github.com/suilin0432/SoS-WSOD">
                        SoS-WSOD,
                       </a>
                    </span>
            </div>
        </div>
    </div> 

    <!-- <section class="section" id="More Research and Related Works">
        <div class="container is-max-desktop content">
          <h2 class="title">More Research and Related Works</h2>
          <span class="author-block" style="font-size: 20px;">
            <a class="navbar-item" href="https://github.com/hustvl/WeakTr">
                WeakTr,
              </a>
              <a class="navbar-item" href="https://github.com/NVlabs/wetectron">
                MIST,
              </a>
              <a class="navbar-item" href="https://github.com/suilin0432/SoS-WSOD">
                SoS-WSOD,
               </a>
            </span>
        </div>
    </section> -->

    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-sm-12">
                <h2>BibTeX</h2>
                <pre><code>
                    @misc{zhu2024weaksamsegmentmeetsweaklysupervised,
                        title={WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition}, 
                        author={Lianghui Zhu and Junwei Zhou and Yan Liu and Xin Hao and Wenyu Liu and Xinggang Wang},
                        year={2024},
                        eprint={2402.14812},
                        archivePrefix={arXiv},
                        primaryClass={cs.CV},
                        url={https://arxiv.org/abs/2402.14812}, 
                  }
                  </code></pre>
            </div>
        </div>
    </div> 

    <!-- <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>
            @misc{zhu2024weaksamsegmentmeetsweaklysupervised,
                title={WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition}, 
                author={Lianghui Zhu and Junwei Zhou and Yan Liu and Xin Hao and Wenyu Liu and Xinggang Wang},
                year={2024},
                eprint={2402.14812},
                archivePrefix={arXiv},
                primaryClass={cs.CV},
                url={https://arxiv.org/abs/2402.14812}, 
          }
          </code></pre>
        </div>
    </section> -->



    <div class="container" style="max-width: 768px;">
        <footer>
            <p> Website template from <a href="https://dreamfusion3d.github.io/">DreamFusion</a>. We thank the authors for the open-source code.</p>
        </footer>
    </div>
    <script src="https://polyfill.io/v3/polyfill.js?features=IntersectionObserver"></script>
    <script src="assets/js/yall.js"></script>
    <script>
        yall(
            {
                observeChanges: true
            }
        );
    </script>
    <script src="assets/js/scripts.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"></script>
    <!-- Import the component -->
</body>

</html>
